import Highlight from '@site/src/components/Highlight'
import Warn from '@site/src/components/Warn'
import Term from '@site/src/components/Term'
import Term2 from '@site/src/components/Term2'

# Sharding

## Introduction
A distributed database distributes data across multiple nodes.
1. Replication: having a copy of the same data on multiple nodes.
2. Sharding: Split up a large database into smaller pieces (shards or partitions). And store different shards on different nodes.

- Sharding is usually combined with replication so that copies of each shard are stored on multiple nodes.

- A node may store more than one shard. Each shard's leader is assigned to one node, and its followers are assigned to other nodes.
![](./assets/data-intensive/sharding.png)

- Shard has many different names. In Kafka, it called partition, called range in CockroachDB.
- In PostgreSQL, it treats partition and shards as two distnct concepts. Partitioning is a way to splitting large table into several files that are stored on the same machine, whereas sharding split a dataset across multiple machines.

## Pros and Cons
- Pros: The primary reason for sharding is scalability, spread the write load across multiple nodes (horizontal scaling).
- Cons: Add complexity as we need to decide which records go to which shard by choosing partiion key.

## Sharding of Key-Value Data
1. Our goal is to spread the data and the query load evenly across multiple nodes.
2. If some shards are more busy than others, we call it *skewed*.
3. A shard with disproportionately high load is called a *hot shard* or *hot spot*.
4. If one key with a particularly high lead (e.g. celebrity in social network), we call it a *hot key*.

### Sharding by Key Range
1. To assign a contiguous range of partition keys to each shard.
2. The range of keys are not necessarily evenly distributed, because your data may not be evenly distributed.
3. The database can choose the shard boundaries.
4. Within each shard, keys are stored in sorted order (e.g., in B-tree or SSTables), make range scan easy.

- Advantage: The number of shards apapts to the data volume.
- Disadvantage: Splitting a shard is expensive since it requires all data to be rewritten into new shards.

#### Rebalancing key-range sharded data
1. Some database, such HBase and MongoDB, allow you to configure an initial set of shards on an empty database, which is called *pre-splitting*.
2. As the data volume and write throughput grows, an existing shard can be shard into two or more smaller shards, each of which holds a contiguous sub-range of the original shard's key range.
3. The resulting smaller shards can then be distributed across the multiple nodes.
4. Shard split is typically triggered by:
    - The shard reaching a configured size (for example, on HBase, 10GB)
    - The write throughput of the shard reaches a configured threshold. A hot shard may be split even if it is not storing a lot of data.

### Sharding by Hash of Key

#### Hash modulo number of nodes
#### Fixed number of shards
#### Sharding by hash range
#### <Term>Consistent Hashing</Term>
- Consistent Hashing algorithm is a hash function that maps keys to a specificied number of shards that satisfies two properties:
    1. The number of keys mapped to each shard is roughly equal.
    2. When the number of shards changes, as few keys as possible are moved from one shard to another.

- Simple approach: Simple Modulo Hashing
    - `database_id = hash(order_id) % num_databases`
    - Problem: Add/Remove database instance costs the data in all databases to be re-distributed as the number of databases changes.

- Consistent Hashing
    - Solve the problem of data redistribution when adding or removing a instance in a distributed system.

    - Start with 4 nodes

    - Adding a Database

    - Removing a Database



### Skewed Workloads and Relieving Hot Spots
### Operations: Automatic or Manual Rebalancing

## Request Routing

## Sharding and Secondary Indexes
